{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:33:47.800259Z",
     "start_time": "2025-06-18T08:32:58.556115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\n",
    "\n",
    "# pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "# generated_ids = model.generate(pixel_values)\n",
    "# generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ],
   "id": "703f3bb9207196d6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "P:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:33:48.426521Z",
     "start_time": "2025-06-18T08:33:48.421893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── 1. tell the model where decoding starts ───────────────────────────────\n",
    "# Roberta-based TrOCR uses <s> as BOS.\n",
    "if processor.tokenizer.bos_token_id is None:\n",
    "    processor.tokenizer.add_special_tokens({\"bos_token\": \"<s>\"})\n",
    "    model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "model.config.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "\n",
    "# ── 2. the other two “must-haves” ─────────────────────────────────────────\n",
    "model.config.eos_token_id  = processor.tokenizer.eos_token_id            # = 2\n",
    "model.config.pad_token_id  = processor.tokenizer.pad_token_id            # = 1\n",
    "\n",
    "# (optional but convenient for generate() and Trainer)\n",
    "model.config.vocab_size    = model.config.decoder.vocab_size\n",
    "model.config.max_length    = 128\n",
    "model.config.num_beams     = 4"
   ],
   "id": "b4ac7ad474f4b46a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:33:48.634667Z",
     "start_time": "2025-06-18T08:33:48.613563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_length=128):\n",
    "        self.data_dir = data_dir\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # find all indexes by looking for .png files\n",
    "        self.ids = sorted([\n",
    "            os.path.splitext(fn)[0]\n",
    "            for fn in os.listdir(data_dir)\n",
    "            if fn.endswith(\".png\")\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_str = self.ids[idx].zfill(len(self) - 1)\n",
    "\n",
    "        img_path = os.path.join(self.data_dir, f\"{idx_str}.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "\n",
    "        txt_path = os.path.join(self.data_dir, f\"{idx_str}.txt\")\n",
    "        caption = open(txt_path, \"r\", encoding=\"utf-8\").read().strip()\n",
    "\n",
    "        tokens = processor.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids.squeeze(0)  # → (max_length,)\n",
    "        # tokens[tokens == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": tokens}\n",
    "\n",
    "train_dataset = ImageTextDataset(\n",
    "    'data', max_length=128\n",
    ")\n",
    "validation_dataset = ImageTextDataset(\n",
    "    'valid', max_length=128\n",
    ")"
   ],
   "id": "fa013a3a3cff9af7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:37:20.088388Z",
     "start_time": "2025-06-18T08:33:48.986578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    TrOCRProcessor,\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")          # use the right metric\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred          # preds may be tuple OR ndarray\n",
    "\n",
    "    # ── 1️⃣  unwrap if we received a tuple ───────────────────────────\n",
    "    if isinstance(preds, tuple):       # case A → logits are preds[0]\n",
    "        preds = preds[0]\n",
    "\n",
    "    # ── 2️⃣  if still 3-D, collapse to token-ids with argmax ─────────\n",
    "    if preds.ndim == 3:                # (batch, seq_len, vocab)\n",
    "        pred_ids = np.argmax(preds, axis=-1)\n",
    "    else:                              # already (batch, seq_len)\n",
    "        pred_ids = preds\n",
    "\n",
    "    # ── 3️⃣  put PAD tokens back into labels so they decode cleanly ─\n",
    "    labels = np.where(labels == -100,\n",
    "                      processor.tokenizer.pad_token_id,\n",
    "                      labels)\n",
    "\n",
    "    # ── 4️⃣  decode and compute CER ─────────────────────────────────\n",
    "    pred_str  = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(labels,   skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str,\n",
    "                             references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}\n",
    "\n",
    "# 5. Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trocr-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    # predict_with_generate=True,\n",
    ")\n",
    "\n",
    "# 6. Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 7. Start training\n",
    "trainer.train()\n",
    "\n",
    "# 8. (Optional) Save the fine‐tuned model\n",
    "trainer.save_model(\"./trocr-finetuned-final\")"
   ],
   "id": "88f0b56c36df5a5a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tamino\\AppData\\Local\\Temp\\ipykernel_3960\\936496020.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1251' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1251/6250 3:03:15 < 12:13:27, 0.11 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'valid\\\\000.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 67\u001B[39m\n\u001B[32m     56\u001B[39m trainer = Trainer(\n\u001B[32m     57\u001B[39m     model=model,\n\u001B[32m     58\u001B[39m     args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m     63\u001B[39m     compute_metrics=compute_metrics,\n\u001B[32m     64\u001B[39m )\n\u001B[32m     66\u001B[39m \u001B[38;5;66;03m# 7. Start training\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[38;5;66;03m# 8. (Optional) Save the fine‐tuned model\u001B[39;00m\n\u001B[32m     70\u001B[39m trainer.save_model(\u001B[33m\"\u001B[39m\u001B[33m./trocr-finetuned-final\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2240\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2238\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2239\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2240\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2241\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2242\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2243\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2244\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2245\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2656\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2653\u001B[39m     \u001B[38;5;28mself\u001B[39m.control.should_training_stop = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   2655\u001B[39m \u001B[38;5;28mself\u001B[39m.control = \u001B[38;5;28mself\u001B[39m.callback_handler.on_epoch_end(args, \u001B[38;5;28mself\u001B[39m.state, \u001B[38;5;28mself\u001B[39m.control)\n\u001B[32m-> \u001B[39m\u001B[32m2656\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_maybe_log_save_evaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2657\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtr_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_time\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlearning_rate\u001B[49m\n\u001B[32m   2658\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2660\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m DebugOption.TPU_METRICS_DEBUG \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.debug:\n\u001B[32m   2661\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_torch_xla_available():\n\u001B[32m   2662\u001B[39m         \u001B[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3095\u001B[39m, in \u001B[36mTrainer._maybe_log_save_evaluate\u001B[39m\u001B[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001B[39m\n\u001B[32m   3093\u001B[39m metrics = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3094\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.control.should_evaluate:\n\u001B[32m-> \u001B[39m\u001B[32m3095\u001B[39m     metrics = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3096\u001B[39m     is_new_best_metric = \u001B[38;5;28mself\u001B[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001B[32m   3098\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3044\u001B[39m, in \u001B[36mTrainer._evaluate\u001B[39m\u001B[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001B[39m\n\u001B[32m   3043\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_evaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m-> \u001B[39m\u001B[32m3044\u001B[39m     metrics = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3045\u001B[39m     \u001B[38;5;28mself\u001B[39m._report_to_hp_search(trial, \u001B[38;5;28mself\u001B[39m.state.global_step, metrics)\n\u001B[32m   3047\u001B[39m     \u001B[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4173\u001B[39m, in \u001B[36mTrainer.evaluate\u001B[39m\u001B[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[39m\n\u001B[32m   4170\u001B[39m start_time = time.time()\n\u001B[32m   4172\u001B[39m eval_loop = \u001B[38;5;28mself\u001B[39m.prediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.use_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.evaluation_loop\n\u001B[32m-> \u001B[39m\u001B[32m4173\u001B[39m output = \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4174\u001B[39m \u001B[43m    \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4175\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdescription\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mEvaluation\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   4176\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;49;00m\n\u001B[32m   4177\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# self.args.prediction_loss_only\u001B[39;49;00m\n\u001B[32m   4178\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   4179\u001B[39m \u001B[43m    \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4180\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4181\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4183\u001B[39m total_batch_size = \u001B[38;5;28mself\u001B[39m.args.eval_batch_size * \u001B[38;5;28mself\u001B[39m.args.world_size\n\u001B[32m   4184\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_key_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_jit_compilation_time\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output.metrics:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4358\u001B[39m, in \u001B[36mTrainer.evaluation_loop\u001B[39m\u001B[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[39m\n\u001B[32m   4355\u001B[39m observed_num_examples = \u001B[32m0\u001B[39m\n\u001B[32m   4357\u001B[39m \u001B[38;5;66;03m# Main evaluation loop\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4358\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   4359\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Update the observed num examples\u001B[39;49;00m\n\u001B[32m   4360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mobserved_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mfind_batch_size\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4361\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobserved_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\accelerate\\data_loader.py:566\u001B[39m, in \u001B[36mDataLoaderShard.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    564\u001B[39m \u001B[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001B[39;00m\n\u001B[32m    565\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m566\u001B[39m     current_batch = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    567\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    568\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    787\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    788\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    791\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 25\u001B[39m, in \u001B[36mImageTextDataset.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m     22\u001B[39m idx_str = \u001B[38;5;28mself\u001B[39m.ids[idx].zfill(\u001B[32m3\u001B[39m)\n\u001B[32m     24\u001B[39m img_path = os.path.join(\u001B[38;5;28mself\u001B[39m.data_dir, \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.png\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m image = \u001B[43mImage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_path\u001B[49m\u001B[43m)\u001B[49m.convert(\u001B[33m\"\u001B[39m\u001B[33mRGB\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     27\u001B[39m pixel_values = processor(images=image, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m).pixel_values.squeeze(\u001B[32m0\u001B[39m)\n\u001B[32m     29\u001B[39m txt_path = os.path.join(\u001B[38;5;28mself\u001B[39m.data_dir, \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.txt\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mP:\\Projects\\HSLU\\dspro2-swiss-receipt-extractor\\.venv\\Lib\\site-packages\\PIL\\Image.py:3505\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(fp, mode, formats)\u001B[39m\n\u001B[32m   3502\u001B[39m     filename = os.fspath(fp)\n\u001B[32m   3504\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[32m-> \u001B[39m\u001B[32m3505\u001B[39m     fp = \u001B[43mbuiltins\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   3506\u001B[39m     exclusive_fp = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   3507\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'valid\\\\000.png'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:37:20.225018100Z",
     "start_time": "2025-06-17T21:33:19.582845Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5a602a7980a52ec7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
