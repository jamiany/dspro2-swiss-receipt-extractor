{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32137815-1b72-45e0-a936-ffd1890108f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DonutModel(\n",
       "  (encoder): SwinEncoder(\n",
       "    (model): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): Sequential(\n",
       "        (0): BasicLayer(\n",
       "          dim=128, input_resolution=(320, 240), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(320, 240), dim=128\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          dim=256, input_resolution=(160, 120), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(160, 120), dim=256\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          dim=512, input_resolution=(80, 60), depth=14\n",
       "          (blocks): ModuleList(\n",
       "            (0-13): 14 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(80, 60), dim=512\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          dim=1024, input_resolution=(40, 30), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): None\n",
       "      (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BARTDecoder(\n",
       "    (model): MBartForCausalLM(\n",
       "      (model): MBartDecoderWrapper(\n",
       "        (decoder): MBartDecoder(\n",
       "          (embed_tokens): Embedding(57540, 1024, padding_idx=1)\n",
       "          (embed_positions): MBartLearnedPositionalEmbedding(770, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0-3): 4 x MBartDecoderLayer(\n",
       "              (self_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=57540, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from transformers.image_utils import load_image\n",
    "from donut import DonutModel\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pretrained_model = DonutModel.from_pretrained(\"result/train_cord/test_experiment\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pretrained_model.half()\n",
    "    pretrained_model.to(\"cuda\")\n",
    "\n",
    "pretrained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ce8383-0b8a-404b-aa50-f969dee5dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import donut\n",
    "json_evalutor = donut.JSONParseEvaluator()\n",
    "\n",
    "def calc_val_edit(image, label):\n",
    "    output = pretrained_model.inference(image=image, prompt=f\"<s_synthetic_data2>\")[\"predictions\"][0]\n",
    "    score = json_evalutor.cal_acc(output, label)\n",
    "    return (score, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7675f74d-7354-4c0f-8c5a-a2ee75a22e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'image'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"test\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ec4265-32e4-4e37-925d-62ba406154a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ted_accuracy': 0.8092537603543895, 'f1_accuracy': 0.7076923076923077}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "scores = []\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for ds in dataset[\"train\"]:\n",
    "    score, prediction = calc_val_edit(ds[\"image\"], ds[\"text\"])\n",
    "    scores.append(score)\n",
    "    predictions.append(prediction)\n",
    "    ground_truths.append(ds[\"text\"])\n",
    "    \n",
    "scores = {\n",
    "    \"ted_accuracy\": numpy.mean(scores),\n",
    "    \"f1_accuracy\": json_evalutor.cal_f1(predictions, ground_truths),\n",
    "}\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1bc61-64be-45c6-b8d2-cb5875fb0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"test_set_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8a7a9d-22fc-4dff-9eeb-33ffc24a8708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ted_accuracy': 0.3177371981700884, 'f1_accuracy': 0.2828282828282828}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for ds in dataset[\"train\"]:\n",
    "    score, prediction = calc_val_edit(ds[\"image\"], ds[\"text\"])\n",
    "    scores.append(score)\n",
    "    predictions.append(prediction)\n",
    "    ground_truths.append(ds[\"text\"])\n",
    "    \n",
    "scores = {\n",
    "    \"ted_accuracy\": numpy.mean(scores),\n",
    "    \"f1_accuracy\": json_evalutor.cal_f1(predictions, ground_truths),\n",
    "}\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc47f107-67e0-4ff1-9d0f-0a371951cd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15de51d33db94724837ce3771d2010a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ted_accuracy': 0.02168674698795181, 'f1_accuracy': 0.03550295857988166}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"test_set_4\")\n",
    "\n",
    "scores = []\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for ds in dataset[\"train\"]:\n",
    "    score, prediction = calc_val_edit(ds[\"image\"], ds[\"text\"])\n",
    "    scores.append(score)\n",
    "    predictions.append(prediction)\n",
    "    ground_truths.append(ds[\"text\"])\n",
    "    \n",
    "scores = {\n",
    "    \"ted_accuracy\": numpy.mean(scores),\n",
    "    \"f1_accuracy\": json_evalutor.cal_f1(predictions, ground_truths),\n",
    "}\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e07b06d-5141-4192-9606-b78370d7e269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ted_accuracy': 0.08902073311057182, 'f1_accuracy': 0.032093362509117436}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import extract_receipt as er\n",
    "import numpy\n",
    "\n",
    "base_dir = 'testdata_with_labels_1'\n",
    "acc_list = []\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for img_name in os.listdir(f'{base_dir}/receipts'):\n",
    "  if not '.jpg' in img_name:\n",
    "    continue\n",
    "\n",
    "  file_num = img_name.replace('.jpg', '')\n",
    "  image = Image.open(f'{base_dir}/receipts/{img_name}').convert(\"RGB\")\n",
    "  try:\n",
    "   image = er.extract_receipt(image)\n",
    "  except:\n",
    "    pass\n",
    "  with open(f'{base_dir}/labels/{file_num}.json', 'r') as f:\n",
    "     label = json.loads(f.read())\n",
    "\n",
    "  score, prediction = calc_val_edit(image, label)\n",
    "  acc_list.append(score)\n",
    "  predictions.append(prediction)\n",
    "  ground_truths.append(label)\n",
    "\n",
    "scores = {\n",
    "    \"ted_accuracy\": numpy.mean(acc_list),\n",
    "    \"f1_accuracy\": json_evalutor.cal_f1(predictions, ground_truths),\n",
    "}\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
